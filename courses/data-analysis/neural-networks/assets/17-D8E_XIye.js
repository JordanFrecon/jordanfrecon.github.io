import{r as o,o as i,c,k as e,l as n,$ as s,q as m,s as p,A as r,e as a}from"./modules/vue-CVPr9In-.js";import{I as h}from"./slidev/default-BnsoQE8Y.js";import{b as d,R as l}from"./index-CTt8qG7m.js";import{p as u,u as _,f as g}from"./slidev/context-CdsOfkDq.js";import"./modules/shiki-DvXQnsHn.js";const f="/courses/data-analysis/neural-networks/chap5/nnet-training-batch2.png",x=a("h1",null,"Backpropagation",-1),y=a("p",null,[s("The training is usually carried on multiple batches of the training data."),a("br"),s(" The parameters are optimized via variants of stochastic gradient descent with step-size (also called learning rate) "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"η"),a("mo",null,">"),a("mn",null,"0")]),a("annotation",{encoding:"application/x-tex"},"\\eta>0")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.7335em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"η"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},">"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6444em"}}),a("span",{class:"mord"},"0")])])]),s(".")],-1),b=a("img",{src:f,style:{height:"240px"}},null,-1),k=a("br",null,null,-1),w={__name:"17",setup(v){return u(l),_(),(T,B)=>{const t=o("center");return i(),c(h,m(p(r(g)(r(l),16))),{default:e(()=>[x,y,n(t,null,{default:e(()=>[b]),_:1}),k,n(t,null,{default:e(()=>[s(" The same process is repeated for the second batch and so on ")]),_:1})]),_:1},16)}}},V=d(w,[["__file","/@slidev/slides/17.md"]]);export{V as default};
