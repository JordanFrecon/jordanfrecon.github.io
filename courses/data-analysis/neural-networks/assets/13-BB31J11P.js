import{r as o,o as l,c as i,k as t,l as c,q as m,s as p,A as e,e as a,$ as s}from"./modules/vue-CVPr9In-.js";import{I as h}from"./slidev/default-BnsoQE8Y.js";import{b as d,N as n}from"./index-CTt8qG7m.js";import{p as u,u as _,f as g}from"./slidev/context-CdsOfkDq.js";import"./modules/shiki-DvXQnsHn.js";const f="/courses/data-analysis/neural-networks/chap5/nnet-training-forward.png",x=a("h1",null,"Backpropagation",-1),w=a("p",null,[s("The training is usually carried on multiple batches of the training data."),a("br"),s(" The parameters are optimized via variants of stochastic gradient descent with step-size (also called learning rate) "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"Î·"),a("mo",null,">"),a("mn",null,"0")]),a("annotation",{encoding:"application/x-tex"},"\\eta>0")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.7335em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"Î·"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},">"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6444em"}}),a("span",{class:"mord"},"0")])])]),s(".")],-1),y=a("img",{src:f,style:{height:"240px"}},null,-1),k={__name:"13",setup(v){return u(n),_(),(b,B)=>{const r=o("center");return l(),i(h,m(p(e(g)(e(n),12))),{default:t(()=>[x,w,c(r,null,{default:t(()=>[y]),_:1})]),_:1},16)}}},P=d(k,[["__file","/@slidev/slides/13.md"]]);export{P as default};
