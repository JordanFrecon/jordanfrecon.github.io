import{r as o,o as i,c,k as t,l as n,$ as s,q as m,s as p,A as r,e as a}from"./modules/vue-CVPr9In-.js";import{I as h}from"./slidev/default-BnsoQE8Y.js";import{b as d,Q as l}from"./index-CTt8qG7m.js";import{p as u,u as _,f as g}from"./slidev/context-CdsOfkDq.js";import"./modules/shiki-DvXQnsHn.js";const f="/courses/data-analysis/neural-networks/chap5/nnet-training-backward.png",x=a("h1",null,"Backpropagation",-1),y=a("p",null,[s("The training is usually carried on multiple batches of the training data."),a("br"),s(" The parameters are optimized via variants of stochastic gradient descent with step-size (also called learning rate) "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"η"),a("mo",null,">"),a("mn",null,"0")]),a("annotation",{encoding:"application/x-tex"},"\\eta>0")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.7335em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"η"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},">"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6444em"}}),a("span",{class:"mord"},"0")])])]),s(".")],-1),k=a("img",{src:f,style:{height:"240px"}},null,-1),w={__name:"16",setup(v){return u(l),_(),(b,B)=>{const e=o("center");return i(),c(h,m(p(r(g)(r(l),15))),{default:t(()=>[x,y,n(e,null,{default:t(()=>[k]),_:1}),n(e,null,{default:t(()=>[s(" And so forth until the first layer ")]),_:1})]),_:1},16)}}},P=d(w,[["__file","/@slidev/slides/16.md"]]);export{P as default};
