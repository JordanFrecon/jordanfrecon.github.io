import{r as o,o as i,c,k as s,l as n,$ as e,q as m,s as p,A as r,e as a}from"./modules/vue-CVPr9In-.js";import{I as h}from"./slidev/default-BnsoQE8Y.js";import{b as u,S as l}from"./index-CTt8qG7m.js";import{p as d,u as _,f as g}from"./slidev/context-CdsOfkDq.js";import"./modules/shiki-DvXQnsHn.js";const f="/courses/data-analysis/neural-networks/chap5/nnet-training-lastbatch.png",x=a("h1",null,"Backpropagation",-1),b=a("p",null,[e("The training is usually carried on multiple batches of the training data."),a("br"),e(" The parameters are optimized via variants of stochastic gradient descent with step-size (also called learning rate) "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"η"),a("mo",null,">"),a("mn",null,"0")]),a("annotation",{encoding:"application/x-tex"},"\\eta>0")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.7335em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"η"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},">"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6444em"}}),a("span",{class:"mord"},"0")])])]),e(".")],-1),k=a("img",{src:f,style:{height:"240px"}},null,-1),v=a("br",null,null,-1),w={__name:"18",setup(y){return d(l),_(),(B,T)=>{const t=o("center");return i(),c(h,m(p(r(g)(r(l),17))),{default:s(()=>[x,b,n(t,null,{default:s(()=>[k]),_:1}),v,n(t,null,{default:s(()=>[e(" Once all batches have been processed, the neural network have been trained for 1 epoch! ")]),_:1})]),_:1},16)}}},V=u(w,[["__file","/@slidev/slides/18.md"]]);export{V as default};
