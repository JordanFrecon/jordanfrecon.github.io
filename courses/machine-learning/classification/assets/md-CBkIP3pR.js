import{a as c,aK as g,b,o as s,w as t,g as l,ai as a,ae as n,f as u,e as o,v,x as h,S as p}from"./modules/vue-_37tKcHB.js";import{_ as x}from"./top-title-BQs-T8Zr.js";import{u as y,f as k}from"./slidev/context-BbfB1rqJ.js";import"./layoutHelper-BN7rjZLy.js";import"./index-B68YNWQ1.js";import"./modules/shiki-bCbkchdg.js";const _={class:"row",style:{"margin-top":"30px"}},w={class:"one-third-col"},P={class:"one-third-col"},B={class:"one-third-col"},K={__name:"ml_4-classification.md__slidev_33",setup(C){const{$clicksContext:d,$frontmatter:m}=y();return d.setup(),(N,e)=>{const i=c("center"),r=g("click");return s(),b(x,v(h(p(k)(p(m),32))),{title:t(f=>e[0]||(e[0]=[l("h1",null,"Conclusion",-1)])),content:t(f=>[e[11]||(e[11]=l("p",null,[n("We’ve explored three fundamental approaches to classification, each with its own "),l("strong",null,"mathematical philosophy"),n(":")],-1)),l("div",_,[a((s(),u("div",w,[o(i,null,{default:t(()=>e[1]||(e[1]=[l("h3",null,[l("strong",null,"Naive Bayes")],-1),l("p",null,[l("strong",null,"Probabilistic Foundation")],-1)])),_:1}),e[3]||(e[3]=l("ul",null,[l("li",null,"Uses Bayes’ theorem"),l("li",null,"Assumes feature independence"),l("li",null,"Direct probability estimation"),l("li",null,"Fast and interpretable")],-1)),o(i,null,{default:t(()=>e[2]||(e[2]=[n(` *"What's the probability of each class given the features?"* `)])),_:1})])),[[r,1]]),a((s(),u("div",P,[o(i,null,{default:t(()=>e[4]||(e[4]=[l("h3",null,[l("strong",null,"Logistic Regression")],-1),l("p",null,[l("strong",null,"Linear + Probabilistic")],-1)])),_:1}),e[6]||(e[6]=l("ul",null,[l("li",null,"Linear combination + sigmoid"),l("li",null,"Maximum likelihood estimation"),l("li",null,"Convex optimization"),l("li",null,"Well-calibrated probabilities")],-1)),o(i,null,{default:t(()=>e[5]||(e[5]=[n(' *"How do features linearly combine to predict class probabilities?"* ')])),_:1})])),[[r,2]]),a((s(),u("div",B,[o(i,null,{default:t(()=>e[7]||(e[7]=[l("h3",null,[l("strong",null,"SVM")],-1),l("p",null,[l("strong",null,"Geometric Approach")],-1)])),_:1}),e[9]||(e[9]=l("ul",null,[l("li",null,"Maximum margin principle"),l("li",null,"Support vector focus"),l("li",null,"Kernel trick for non-linearity"),l("li",null,"Robust generalization")],-1)),o(i,null,{default:t(()=>e[8]||(e[8]=[n(` *"What's the best separating boundary with maximum margin?"* `)])),_:1})])),[[r,3]])]),a((s(),u("div",null,[o(i,null,{default:t(()=>e[10]||(e[10]=[l("p",null,[l("strong",null,"Key Takeaway"),n(': There’s no "best" classifier - the choice depends on your '),l("strong",null,"data characteristics"),n(", "),l("strong",null,"interpretability needs"),n(", and "),l("strong",null,"performance requirements"),n(".")],-1),l("br",null,null,-1),l("p",null,[l("strong",null,"Next Steps"),n(": Understanding these fundamentals prepares you for more advanced methods like ensemble learning, neural networks, and deep learning - but these three approaches remain the "),l("strong",null,"building blocks"),n(" of modern classification!")],-1)])),_:1})])),[[r,4]])]),_:1},16)}}};export{K as default};
