import{a as u,b as p,o as m,w as n,g as t,e as d,ae as e,v as g,x as f,X as s}from"./modules/vue-DQ9iuSIT.js";import{_ as c}from"./top-title-B7sELMjj.js";import{u as x,f as v}from"./slidev/context-jNdlTRHP.js";import"./layoutHelper-HtdtuDyr.js";import"./index-CptFy4Cl.js";import"./monaco/bundled-types-C8a_JgbF.js";import"./modules/file-saver-BzOOqXCn.js";import"./modules/shiki-Dxkq1cMF.js";const V={__name:"datascience_5-vectorize-dimension.md__slidev_48",setup(_){const{$clicksContext:r,$frontmatter:l}=x();return r.setup(),(b,o)=>{const i=u("center");return m(),p(c,g(f(s(v)(s(l),47))),{title:n(a=>o[0]||(o[0]=[t("h1",null,"To Go Further with Vectorization",-1)])),content:n(a=>[o[2]||(o[2]=t("p",null,[e("Bag of Words is just the "),t("strong",null,"first step"),e(" ðŸ“¦")],-1)),o[3]||(o[3]=t("p",null,"In more advanced modules, youâ€™ll discover:",-1)),o[4]||(o[4]=t("ul",null,[t("li",null,[t("strong",null,"Tokenizers"),e(": splitting text into subwords or characters (not just words)")]),t("li",null,[t("strong",null,"Embeddings"),e(": mapping tokens into dense vectors that capture meaning")]),t("li",null,[t("strong",null,"Language Models"),e(": using billions of parameters to learn context and semantics")])],-1)),o[5]||(o[5]=t("p",null,[t("br"),t("br")],-1)),d(i,null,{default:n(()=>o[1]||(o[1]=[t("p",null,[e("This is the foundation of modern "),t("strong",null,"LLMs"),e(" (Large Language Models) like ChatGPT")],-1)])),_:1})]),_:1},16)}}};export{V as default};
